{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Dubbing Service V2 - Google Colab Edition\n",
        "\n",
        "This notebook allows you to run the video dubbing service on Google Colab with GPU support.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Upload this notebook to Google Colab\n",
        "2. Upload your `v2` project folder to Colab (or clone from repository)\n",
        "3. Mount Google Drive (optional - for saving files)\n",
        "4. Upload your video files to `/content/videos/` (or use the upload widget)\n",
        "5. Run all cells sequentially\n",
        "6. Process your videos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.10.2 Requires-Python >=3.6.0, <3.9; 0.0.10.3 Requires-Python >=3.6.0, <3.9; 0.0.11 Requires-Python >=3.6.0, <3.9; 0.0.12 Requires-Python >=3.6.0, <3.9; 0.0.13.1 Requires-Python >=3.6.0, <3.9; 0.0.13.2 Requires-Python >=3.6.0, <3.9; 0.0.14.1 Requires-Python >=3.6.0, <3.9; 0.0.15 Requires-Python >=3.6.0, <3.9; 0.0.15.1 Requires-Python >=3.6.0, <3.9; 0.0.9 Requires-Python >=3.6.0, <3.9; 0.0.9.1 Requires-Python >=3.6.0, <3.9; 0.0.9.2 Requires-Python >=3.6.0, <3.9; 0.0.9a10 Requires-Python >=3.6.0, <3.9; 0.0.9a9 Requires-Python >=3.6.0, <3.9; 0.1.0 Requires-Python >=3.6.0, <3.10; 0.1.1 Requires-Python >=3.6.0, <3.10; 0.1.2 Requires-Python >=3.6.0, <3.10; 0.1.3 Requires-Python >=3.6.0, <3.10; 0.10.0 Requires-Python >=3.7.0, <3.11; 0.10.1 Requires-Python >=3.7.0, <3.11; 0.10.2 Requires-Python >=3.7.0, <3.11; 0.11.0 Requires-Python >=3.7.0, <3.11; 0.11.1 Requires-Python >=3.7.0, <3.11; 0.12.0 Requires-Python >=3.7.0, <3.11; 0.13.0 Requires-Python >=3.7.0, <3.11; 0.13.1 Requires-Python >=3.7.0, <3.11; 0.13.2 Requires-Python >=3.7.0, <3.11; 0.13.3 Requires-Python >=3.7.0, <3.11; 0.14.0 Requires-Python >=3.7.0, <3.11; 0.14.2 Requires-Python >=3.7.0, <3.11; 0.14.3 Requires-Python >=3.7.0, <3.11; 0.15.0 Requires-Python >=3.9.0, <3.12; 0.15.1 Requires-Python >=3.9.0, <3.12; 0.15.2 Requires-Python >=3.9.0, <3.12; 0.15.4 Requires-Python >=3.9.0, <3.12; 0.15.5 Requires-Python >=3.9.0, <3.12; 0.15.6 Requires-Python >=3.9.0, <3.12; 0.16.0 Requires-Python >=3.9.0, <3.12; 0.16.1 Requires-Python >=3.9.0, <3.12; 0.16.3 Requires-Python >=3.9.0, <3.12; 0.16.4 Requires-Python >=3.9.0, <3.12; 0.16.5 Requires-Python >=3.9.0, <3.12; 0.16.6 Requires-Python >=3.9.0, <3.12; 0.17.0 Requires-Python >=3.9.0, <3.12; 0.17.1 Requires-Python >=3.9.0, <3.12; 0.17.2 Requires-Python >=3.9.0, <3.12; 0.17.4 Requires-Python >=3.9.0, <3.12; 0.17.5 Requires-Python >=3.9.0, <3.12; 0.17.6 Requires-Python >=3.9.0, <3.12; 0.17.7 Requires-Python >=3.9.0, <3.12; 0.17.8 Requires-Python >=3.9.0, <3.12; 0.17.9 Requires-Python >=3.9.0, <3.12; 0.18.0 Requires-Python >=3.9.0, <3.12; 0.18.1 Requires-Python >=3.9.0, <3.12; 0.18.2 Requires-Python >=3.9.0, <3.12; 0.19.0 Requires-Python >=3.9.0, <3.12; 0.19.1 Requires-Python >=3.9.0, <3.12; 0.2.0 Requires-Python >=3.6.0, <3.10; 0.2.1 Requires-Python >=3.6.0, <3.10; 0.2.2 Requires-Python >=3.6.0, <3.10; 0.20.0 Requires-Python >=3.9.0, <3.12; 0.20.1 Requires-Python >=3.9.0, <3.12; 0.20.2 Requires-Python >=3.9.0, <3.12; 0.20.3 Requires-Python >=3.9.0, <3.12; 0.20.4 Requires-Python >=3.9.0, <3.12; 0.20.5 Requires-Python >=3.9.0, <3.12; 0.20.6 Requires-Python >=3.9.0, <3.12; 0.21.0 Requires-Python >=3.9.0, <3.12; 0.21.1 Requires-Python >=3.9.0, <3.12; 0.21.2 Requires-Python >=3.9.0, <3.12; 0.21.3 Requires-Python >=3.9.0, <3.12; 0.22.0 Requires-Python >=3.9.0, <3.12; 0.3.0 Requires-Python >=3.6.0, <3.10; 0.3.1 Requires-Python >=3.6.0, <3.10; 0.4.0 Requires-Python >=3.6.0, <3.10; 0.4.1 Requires-Python >=3.6.0, <3.10; 0.4.2 Requires-Python >=3.6.0, <3.10; 0.5.0 Requires-Python >=3.6.0, <3.10; 0.6.0 Requires-Python >=3.6.0, <3.10; 0.6.1 Requires-Python >=3.6.0, <3.10; 0.6.2 Requires-Python >=3.6.0, <3.10; 0.7.0 Requires-Python >=3.7.0, <3.11; 0.7.1 Requires-Python >=3.7.0, <3.11; 0.8.0 Requires-Python >=3.7.0, <3.11; 0.9.0 Requires-Python >=3.7.0, <3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement TTS (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for TTS\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ Dependencies installed successfully!\n",
            "‚ö†Ô∏è  Make sure ffmpeg and sox are installed on your system\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q torch torchaudio transformers openai-whisper demucs moviepy pydub pyrubberband python-dotenv TTS\n",
        "\n",
        "# Install system dependencies for audio processing\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg sox\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error: Could not find v2 directory!\n",
            "   Current directory: /content\n",
            "   Please ensure you're running from the project directory\n",
            "   Expected structure: .../Dubbing-Service/v2/services/\n"
          ]
        }
      ],
      "source": [
        "# Setup environment and paths\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive (optional - if you want to save files there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory to /content\n",
        "WORK_DIR = Path('/content')\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f\"‚úÖ Environment setup complete\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Python path includes: []\n",
            "‚ùå Import error: No module named 'v2'\n",
            "\n",
            "üîß Troubleshooting:\n",
            "   1. Make sure Cell 2 ran successfully\n",
            "   2. Check that sys.path includes the project root directory\n",
            "   3. Verify v2/services/ directory exists with all required files\n",
            "\n",
            "   Current sys.path entries related to project:\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'v2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3851084901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# First verify v2 can be imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ 'v2' module location: {v2.__file__ if hasattr(v2, '__file__') else 'built-in'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'v2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries and setup logging\n",
        "import asyncio\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import uuid\n",
        "import io\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Audio/Video processing\n",
        "from moviepy import VideoFileClip, AudioFileClip\n",
        "from pydub import AudioSegment\n",
        "import soundfile as sf\n",
        "\n",
        "# ML/AI libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "try:\n",
        "    import whisper\n",
        "    WHISPER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WHISPER_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from demucs import pretrained\n",
        "    from demucs.apply import apply_model\n",
        "    DEMUCS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DEMUCS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoProcessor, AutoModel\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from TTS.api import TTS as CoquiTTS\n",
        "    COQUI_TTS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    COQUI_TTS_AVAILABLE = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Classes and Functions\n",
        "\n",
        "The cells below contain all necessary service classes and functions for video dubbing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation Tools - NLLB Translator\n",
        "NLLB_LANGUAGE_MAP = {\n",
        "    'en': 'eng_Latn', 'ha': 'hau_Latn', 'ig': 'ibo_Latn', 'yo': 'yor_Latn',\n",
        "    'fr': 'fra_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn', 'ru': 'rus_Cyrl',\n",
        "    'zh': 'zho_Hans', 'sw': 'swh_Latn',\n",
        "}\n",
        "NLLB_MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "\n",
        "class NLLBTranslator:\n",
        "    _model = None\n",
        "    _tokenizer = None\n",
        "    _device = None\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_device(cls):\n",
        "        if cls._device:\n",
        "            return cls._device\n",
        "        if torch.cuda.is_available():\n",
        "            cls._device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            cls._device = torch.device(\"cpu\")\n",
        "        return cls._device\n",
        "    \n",
        "    @classmethod\n",
        "    def _load_model(cls):\n",
        "        if cls._model is not None:\n",
        "            return cls._model, cls._tokenizer\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            raise ImportError(\"transformers not available\")\n",
        "        logger.info(f\"Loading NLLB model: {NLLB_MODEL_NAME}\")\n",
        "        device = cls._get_device()\n",
        "        cls._tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL_NAME, src_lang=\"eng_Latn\")\n",
        "        cls._model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL_NAME).to(device)\n",
        "        cls._model.eval()\n",
        "        return cls._model, cls._tokenizer\n",
        "    \n",
        "    @classmethod\n",
        "    def translate(cls, text: str, source_language: str, target_language: str, max_length: int = 512) -> str:\n",
        "        if not text or not text.strip():\n",
        "            return text\n",
        "        model, tokenizer = cls._load_model()\n",
        "        device = cls._get_device()\n",
        "        src_lang_code = NLLB_LANGUAGE_MAP.get(source_language.lower(), 'eng_Latn')\n",
        "        tgt_lang_code = NLLB_LANGUAGE_MAP.get(target_language.lower(), 'eng_Latn')\n",
        "        tokenizer.src_lang = src_lang_code\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang_code], max_length=max_length, num_beams=4, early_stopping=True)\n",
        "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "        return translated_text.strip()\n",
        "\n",
        "print(\"‚úÖ Translation tools loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS Tools - Coqui XTTS and helpers\n",
        "TTS_LANGUAGE_MAP = {'en': 'en', 'ha': 'ha', 'ig': 'ig', 'yo': 'yo', 'fr': 'fr', 'es': 'es', 'de': 'de', 'ru': 'ru', 'zh': 'zh', 'sw': 'sw'}\n",
        "COQUI_XTTS_MODEL = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "\n",
        "class CoquiXTTS:\n",
        "    _tts_model = None\n",
        "    _device = None\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_device(cls):\n",
        "        if cls._device:\n",
        "            return cls._device\n",
        "        cls._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return cls._device\n",
        "    \n",
        "    @classmethod\n",
        "    def _load_model(cls):\n",
        "        if cls._tts_model is not None:\n",
        "            return cls._tts_model\n",
        "        if not COQUI_TTS_AVAILABLE:\n",
        "            raise ImportError(\"Coqui TTS not available\")\n",
        "        logger.info(\"Loading Coqui XTTS-v2 model\")\n",
        "        cls._tts_model = CoquiTTS(model_name=COQUI_XTTS_MODEL, progress_bar=False)\n",
        "        cls._tts_model.to(cls._get_device())\n",
        "        return cls._tts_model\n",
        "    \n",
        "    @classmethod\n",
        "    def synthesize(cls, text: str, language: str, speaker_wav: Optional[str] = None) -> AudioSegment:\n",
        "        if not text or not text.strip():\n",
        "            raise ValueError(\"Empty text\")\n",
        "        tts_model = cls._load_model()\n",
        "        lang_code = TTS_LANGUAGE_MAP.get(language.lower(), 'en')\n",
        "        output_path = os.path.join(tempfile.gettempdir(), f\"temp_tts_{hash(text) % 10000}.wav\")\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        try:\n",
        "            if speaker_wav and os.path.exists(speaker_wav):\n",
        "                tts_model.tts_to_file(text=text, file_path=output_path, language=lang_code, speaker_wav=speaker_wav)\n",
        "            else:\n",
        "                try:\n",
        "                    tts_model.tts_to_file(text=text, file_path=output_path, language=lang_code)\n",
        "                except:\n",
        "                    tts_model.tts_to_file(text=text, file_path=output_path)\n",
        "            audio = AudioSegment.from_wav(output_path)\n",
        "            try:\n",
        "                os.remove(output_path)\n",
        "            except:\n",
        "                pass\n",
        "            return audio\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Coqui TTS failed: {e}\")\n",
        "            raise\n",
        "\n",
        "def generate_tts_huggingface(text: str, language: str, prefer_coqui: bool = True) -> AudioSegment:\n",
        "    lang = language.lower()\n",
        "    if prefer_coqui and COQUI_TTS_AVAILABLE:\n",
        "        return CoquiXTTS.synthesize(text, language)\n",
        "    else:\n",
        "        raise ImportError(\"No TTS available. Install: pip install TTS\")\n",
        "\n",
        "print(\"‚úÖ TTS tools loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Service Classes - All service processors inline\n",
        "\n",
        "class VideoProcessor:\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self._demucs_model = None\n",
        "    \n",
        "    async def extract_audio(self, video_path: str) -> str:\n",
        "        logger.info(f\"[JOB {self.job_id}] Extracting audio\")\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is None:\n",
        "            raise Exception(\"No audio track found\")\n",
        "        audio_path = os.path.join(output_dir, \"original_audio.wav\")\n",
        "        audio.write_audiofile(audio_path, fps=22050, nbytes=2, logger=None)\n",
        "        test_audio = AudioSegment.from_file(audio_path)\n",
        "        if test_audio.channels == 1:\n",
        "            stereo_audio = test_audio.set_channels(2)\n",
        "            stereo_audio.export(audio_path, format=\"wav\")\n",
        "        audio.close()\n",
        "        video.close()\n",
        "        return audio_path\n",
        "    \n",
        "    async def separate_audio(self, audio_path: str) -> Dict[str, str]:\n",
        "        if not DEMUCS_AVAILABLE:\n",
        "            raise Exception(\"Demucs not available\")\n",
        "        if self._demucs_model is None:\n",
        "            self._demucs_model = pretrained.get_model('htdemucs')\n",
        "            self._demucs_model.eval()\n",
        "        wav, sr = torchaudio.load(audio_path)\n",
        "        if sr != self._demucs_model.samplerate:\n",
        "            wav = torchaudio.functional.resample(wav, sr, self._demucs_model.samplerate)\n",
        "            sr = self._demucs_model.samplerate\n",
        "        if wav.dim() == 2:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        elif wav.dim() == 1:\n",
        "            wav = wav.unsqueeze(0).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            sources = apply_model(self._demucs_model, wav, device='cpu', progress=True)\n",
        "        source_names = ['drums', 'bass', 'other', 'vocals']\n",
        "        separated_files = {}\n",
        "        output_dir = os.path.dirname(audio_path)\n",
        "        for i, name in enumerate(source_names):\n",
        "            output_path = os.path.join(output_dir, f\"{name}.wav\")\n",
        "            source_audio = sources[0, i].cpu().numpy()\n",
        "            if len(source_audio.shape) == 1:\n",
        "                source_audio = np.stack([source_audio, source_audio])\n",
        "            sf.write(output_path, source_audio.T, sr)\n",
        "            separated_files[name] = output_path\n",
        "        return separated_files\n",
        "    \n",
        "    async def create_background(self, separated_files: Dict[str, str]) -> str:\n",
        "        output_dir = os.path.dirname(separated_files['drums'])\n",
        "        background_path = os.path.join(output_dir, \"background.wav\")\n",
        "        drums, sr = sf.read(separated_files['drums'])\n",
        "        bass, _ = sf.read(separated_files['bass'])\n",
        "        other, _ = sf.read(separated_files['other'])\n",
        "        background = drums + bass + other\n",
        "        sf.write(background_path, background, sr)\n",
        "        return background_path\n",
        "    \n",
        "    async def replace_audio(self, video_path: str, audio_path: str) -> str:\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, \"dubbed_video.mp4\")\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio_clip = AudioFileClip(audio_path)\n",
        "        if audio_clip.duration < video.duration:\n",
        "            audio_clip = audio_clip.set_duration(video.duration)\n",
        "        elif audio_clip.duration > video.duration:\n",
        "            audio_clip = audio_clip.subclipped(0, video.duration)\n",
        "        final_video = video.with_audio(audio_clip)\n",
        "        final_video.write_videofile(output_path, codec='libx264', audio_codec='aac', logger=None)\n",
        "        video.close()\n",
        "        audio_clip.close()\n",
        "        final_video.close()\n",
        "        return output_path\n",
        "\n",
        "class TranscriptionProcessor:\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self._whisper_model = None\n",
        "    \n",
        "    async def transcribe(self, audio_path: str, language: str) -> List[Dict[str, Any]]:\n",
        "        if not WHISPER_AVAILABLE:\n",
        "            raise Exception(\"Whisper not available\")\n",
        "        if self._whisper_model is None:\n",
        "            self._whisper_model = whisper.load_model(\"base\")\n",
        "        result = self._whisper_model.transcribe(audio_path, language=language if language != \"en\" else None, task=\"transcribe\", fp16=False)\n",
        "        segments = []\n",
        "        for seg in result.get(\"segments\", []):\n",
        "            segments.append({\"start_time\": seg.get(\"start\", 0), \"end_time\": seg.get(\"end\", 0), \"duration\": seg.get(\"end\", 0) - seg.get(\"start\", 0), \"transcription\": seg.get(\"text\", \"\").strip()})\n",
        "        return segments\n",
        "\n",
        "class TranslationProcessor:\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "    \n",
        "    async def translate(self, segments: List[Dict[str, Any]], source_language: str, target_language: str) -> List[Dict[str, Any]]:\n",
        "        translated_segments = []\n",
        "        for seg in segments:\n",
        "            text = seg.get(\"transcription\", \"\")\n",
        "            if not text:\n",
        "                seg[\"translated_text\"] = \"\"\n",
        "                translated_segments.append(seg)\n",
        "                continue\n",
        "            if target_language in ['yo', 'ig', 'ha'] and TRANSFORMERS_AVAILABLE:\n",
        "                try:\n",
        "                    translated_text = NLLBTranslator.translate(text, source_language, target_language)\n",
        "                except:\n",
        "                    translated_text = text\n",
        "            else:\n",
        "                translated_text = text\n",
        "            seg[\"translated_text\"] = translated_text\n",
        "            translated_segments.append(seg)\n",
        "        return translated_segments\n",
        "\n",
        "class TTSProcessor:\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "    \n",
        "    async def generate_tts(self, segments: List[Dict[str, Any]], target_language: str) -> List[Dict[str, Any]]:\n",
        "        tts_segments = []\n",
        "        for i, seg in enumerate(segments):\n",
        "            text = seg.get(\"translated_text\", \"\")\n",
        "            if not text:\n",
        "                seg[\"audio\"] = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "                tts_segments.append(seg)\n",
        "                continue\n",
        "            if target_language in ['yo', 'ig', 'ha'] and COQUI_TTS_AVAILABLE:\n",
        "                try:\n",
        "                    audio = generate_tts_huggingface(text, target_language, prefer_coqui=True)\n",
        "                except:\n",
        "                    audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            elif COQUI_TTS_AVAILABLE:\n",
        "                try:\n",
        "                    audio = CoquiXTTS.synthesize(text, target_language)\n",
        "                except:\n",
        "                    audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            else:\n",
        "                audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            seg[\"audio\"] = audio\n",
        "            tts_segments.append(seg)\n",
        "        return tts_segments\n",
        "\n",
        "class AudioAssembler:\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self.MIN_PAUSE_BETWEEN_SEGMENTS = 0.3\n",
        "        self.IDEAL_PAUSE_BETWEEN_SEGMENTS = 0.5\n",
        "        self.FADE_IN_DURATION = 50\n",
        "        self.FADE_OUT_DURATION = 100\n",
        "    \n",
        "    def _add_fade_effects(self, audio: AudioSegment) -> AudioSegment:\n",
        "        return audio.fade_in(self.FADE_IN_DURATION).fade_out(self.FADE_OUT_DURATION)\n",
        "    \n",
        "    def _add_natural_pause(self, audio: AudioSegment, pause_duration_ms: int) -> AudioSegment:\n",
        "        if pause_duration_ms > 0:\n",
        "            return audio + AudioSegment.silent(duration=pause_duration_ms)\n",
        "        return audio\n",
        "    \n",
        "    def _calculate_non_overlapping_positions(self, tts_segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        positioned_segments = []\n",
        "        current_end_time = 0.0\n",
        "        for i, seg in enumerate(tts_segments):\n",
        "            if \"audio\" not in seg:\n",
        "                positioned_segments.append(seg)\n",
        "                continue\n",
        "            audio = seg[\"audio\"]\n",
        "            audio_duration = len(audio) / 1000.0\n",
        "            original_start = seg.get(\"start_time\", 0)\n",
        "            if i == 0:\n",
        "                final_start = max(0.0, original_start)\n",
        "            else:\n",
        "                min_start = current_end_time + self.MIN_PAUSE_BETWEEN_SEGMENTS\n",
        "                final_start = max(original_start, min_start)\n",
        "            pause_duration_ms = int(self.IDEAL_PAUSE_BETWEEN_SEGMENTS * 1000)\n",
        "            audio_with_pause = self._add_natural_pause(audio, pause_duration_ms)\n",
        "            seg_copy = seg.copy()\n",
        "            seg_copy[\"audio\"] = audio_with_pause\n",
        "            seg_copy[\"final_start_time\"] = final_start\n",
        "            seg_copy[\"final_end_time\"] = final_start + (len(audio_with_pause) / 1000.0)\n",
        "            positioned_segments.append(seg_copy)\n",
        "            current_end_time = seg_copy[\"final_end_time\"]\n",
        "        return positioned_segments\n",
        "    \n",
        "    async def assemble(self, tts_segments: List[Dict[str, Any]], background_path: str) -> str:\n",
        "        background = AudioSegment.from_file(background_path)\n",
        "        background_duration = len(background) / 1000.0\n",
        "        positioned_segments = self._calculate_non_overlapping_positions(tts_segments)\n",
        "        max_end_time = 0.0\n",
        "        for seg in positioned_segments:\n",
        "            end_time = seg.get(\"final_end_time\", seg.get(\"end_time\", 0))\n",
        "            max_end_time = max(max_end_time, end_time)\n",
        "        total_duration = max(background_duration, max_end_time)\n",
        "        total_duration_ms = int(total_duration * 1000)\n",
        "        assembled_audio = AudioSegment.silent(duration=total_duration_ms)\n",
        "        assembled_audio = assembled_audio.overlay(background, gain_during_overlay=-6)\n",
        "        for seg in positioned_segments:\n",
        "            if \"audio\" not in seg:\n",
        "                continue\n",
        "            audio = seg[\"audio\"]\n",
        "            final_start = seg.get(\"final_start_time\", seg.get(\"start_time\", 0))\n",
        "            audio = self._add_fade_effects(audio)\n",
        "            start_ms = int(final_start * 1000)\n",
        "            if start_ms + len(audio) > total_duration_ms:\n",
        "                max_audio_length = total_duration_ms - start_ms\n",
        "                if max_audio_length > 0:\n",
        "                    audio = audio[:max_audio_length]\n",
        "                else:\n",
        "                    continue\n",
        "            assembled_audio = assembled_audio.overlay(audio, position=start_ms, gain_during_overlay=+3)\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        final_audio_path = os.path.join(output_dir, \"final_audio.mp3\")\n",
        "        assembled_audio.export(final_audio_path, format=\"mp3\", bitrate=\"320k\")\n",
        "        return final_audio_path\n",
        "\n",
        "print(\"‚úÖ Service classes loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DubbingProcessor - Main processor class\n",
        "class DubbingProcessor:\n",
        "    def __init__(self):\n",
        "        self.job_id = f\"job_{uuid.uuid4().hex[:12]}\"\n",
        "        self.temp_files = []\n",
        "        \n",
        "    async def process_video(self, video_path: str, source_language: str, target_language: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            logger.info(f\"[JOB {self.job_id}] Starting dubbing: {video_path}\")\n",
        "            logger.info(f\"[JOB {self.job_id}] Languages: {source_language} ‚Üí {target_language}\")\n",
        "            \n",
        "            # Phase 1: Extract audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 1: Extracting audio\")\n",
        "            video_processor = VideoProcessor(self.job_id)\n",
        "            audio_path = await video_processor.extract_audio(video_path)\n",
        "            self.temp_files.append(audio_path)\n",
        "            \n",
        "            # Phase 2: Separate audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 2: Separating audio\")\n",
        "            separated_files = await video_processor.separate_audio(audio_path)\n",
        "            self.temp_files.extend(separated_files.values())\n",
        "            \n",
        "            # Phase 3: Create background\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 3: Creating background\")\n",
        "            background_path = await video_processor.create_background(separated_files)\n",
        "            self.temp_files.append(background_path)\n",
        "            \n",
        "            # Phase 4: Transcribe\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 4: Transcribing\")\n",
        "            transcription_processor = TranscriptionProcessor(self.job_id)\n",
        "            segments = await transcription_processor.transcribe(separated_files['vocals'], source_language)\n",
        "            \n",
        "            # Phase 5: Translate\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 5: Translating\")\n",
        "            translation_processor = TranslationProcessor(self.job_id)\n",
        "            translated_segments = await translation_processor.translate(segments, source_language, target_language)\n",
        "            \n",
        "            # Phase 6: Generate TTS\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 6: Generating TTS\")\n",
        "            tts_processor = TTSProcessor(self.job_id)\n",
        "            tts_segments = await tts_processor.generate_tts(translated_segments, target_language)\n",
        "            \n",
        "            # Phase 7: Assemble audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 7: Assembling audio\")\n",
        "            assembler = AudioAssembler(self.job_id)\n",
        "            final_audio_path = await assembler.assemble(tts_segments, background_path)\n",
        "            self.temp_files.append(final_audio_path)\n",
        "            \n",
        "            # Phase 8: Replace video audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 8: Replacing video audio\")\n",
        "            output_path = await video_processor.replace_audio(video_path, final_audio_path)\n",
        "            \n",
        "            logger.info(f\"[JOB {self.job_id}] ‚úÖ Dubbing complete: {output_path}\")\n",
        "            return {\"success\": True, \"output_path\": output_path, \"job_id\": self.job_id}\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[JOB {self.job_id}] ‚ùå Dubbing failed: {e}\", exc_info=True)\n",
        "            return {\"success\": False, \"error\": str(e), \"job_id\": self.job_id}\n",
        "\n",
        "print(\"‚úÖ DubbingProcessor loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_video_directory(\n",
        "    video_directory: str,\n",
        "    source_language: str,\n",
        "    target_language: str,\n",
        "    output_directory: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Process all videos in a directory through the dubbing pipeline\n",
        "    \n",
        "    Args:\n",
        "        video_directory: Path to directory containing video files\n",
        "        source_language: Source language code (e.g., 'en', 'fr', 'es')\n",
        "        target_language: Target language code (e.g., 'yo', 'ig', 'ha')\n",
        "        output_directory: Optional output directory (defaults to video_directory/dubbed)\n",
        "    \n",
        "    Returns:\n",
        "        List of results for each processed video\n",
        "    \"\"\"\n",
        "    video_dir = Path(video_directory)\n",
        "    if not video_dir.exists():\n",
        "        raise ValueError(f\"Video directory not found: {video_directory}\")\n",
        "    \n",
        "    # Set output directory\n",
        "    if output_directory is None:\n",
        "        output_dir = video_dir / \"dubbed\"\n",
        "    else:\n",
        "        output_dir = Path(output_directory)\n",
        "    \n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Find all video files\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v']\n",
        "    video_files = []\n",
        "    for ext in video_extensions:\n",
        "        video_files.extend(video_dir.glob(f\"*{ext}\"))\n",
        "        video_files.extend(video_dir.glob(f\"*{ext.upper()}\"))\n",
        "    \n",
        "    if not video_files:\n",
        "        logger.warning(f\"No video files found in {video_directory}\")\n",
        "        return []\n",
        "    \n",
        "    logger.info(f\"Found {len(video_files)} video file(s) to process\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Process each video\n",
        "    for video_path in video_files:\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Processing: {video_path.name}\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "        \n",
        "        try:\n",
        "            # Create dubbing processor\n",
        "            processor = DubbingProcessor()\n",
        "            \n",
        "            # Process video\n",
        "            result = await processor.process_video(\n",
        "                str(video_path),\n",
        "                source_language,\n",
        "                target_language\n",
        "            )\n",
        "            \n",
        "            if result['success']:\n",
        "                # Move output to designated output directory\n",
        "                output_path = Path(result['output_path'])\n",
        "                final_output = output_dir / f\"{video_path.stem}_dubbed{video_path.suffix}\"\n",
        "                \n",
        "                # Copy or move the file\n",
        "                import shutil\n",
        "                shutil.copy2(output_path, final_output)\n",
        "                logger.info(f\"‚úÖ Saved dubbed video to: {final_output}\")\n",
        "                \n",
        "                result['final_output_path'] = str(final_output)\n",
        "                results.append(result)\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Failed to process {video_path.name}: {result.get('error', 'Unknown error')}\")\n",
        "                results.append(result)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error processing {video_path.name}: {e}\", exc_info=True)\n",
        "            results.append({\n",
        "                'success': False,\n",
        "                'video': str(video_path),\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Processing complete! Processed {len([r for r in results if r.get('success')])}/{len(video_files)} videos\")\n",
        "    logger.info(f\"Output directory: {output_dir}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def process_videos(\n",
        "    video_directory: str,\n",
        "    source_language: str,\n",
        "    target_language: str,\n",
        "    output_directory: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Synchronous wrapper for process_video_directory\n",
        "    \n",
        "    Use this function directly in Colab cells\n",
        "    \"\"\"\n",
        "    # Run async function\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        results = loop.run_until_complete(\n",
        "            process_video_directory(\n",
        "                video_directory,\n",
        "                source_language,\n",
        "                target_language,\n",
        "                output_directory\n",
        "            )\n",
        "        )\n",
        "        return results\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Dubbing functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION - Adjust these parameters\n",
        "\n",
        "# Path to directory containing your video files\n",
        "VIDEO_DIRECTORY = \"/content/videos\"  # Change this to your video directory path\n",
        "\n",
        "# Source language (language spoken in the video)\n",
        "SOURCE_LANGUAGE = \"en\"  # Options: 'en', 'fr', 'es', 'de', 'ru', 'zh', etc.\n",
        "\n",
        "# Target language (language you want to dub to)\n",
        "TARGET_LANGUAGE = \"yo\"  # Options: 'yo' (Yoruba), 'ig' (Igbo), 'ha' (Hausa), etc.\n",
        "\n",
        "# Output directory (optional - will create 'dubbed' subdirectory if not specified)\n",
        "OUTPUT_DIRECTORY = None  # e.g., \"/content/drive/MyDrive/dubbed_videos\"\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Video Directory: {VIDEO_DIRECTORY}\")\n",
        "print(f\"  Source Language: {SOURCE_LANGUAGE}\")\n",
        "print(f\"  Target Language: {TARGET_LANGUAGE}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIRECTORY or 'Auto (dubbed subdirectory)'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create video directory if it doesn't exist\n",
        "os.makedirs(VIDEO_DIRECTORY, exist_ok=True)\n",
        "print(f\"üìÅ Video directory ready: {VIDEO_DIRECTORY}\")\n",
        "\n",
        "# List existing video files\n",
        "video_files = []\n",
        "if os.path.exists(VIDEO_DIRECTORY):\n",
        "    video_files = [f for f in os.listdir(VIDEO_DIRECTORY) \n",
        "                   if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v'))]\n",
        "\n",
        "if video_files:\n",
        "    print(f\"üìπ Found {len(video_files)} video file(s) in {VIDEO_DIRECTORY}:\")\n",
        "    for vf in video_files[:5]:  # Show first 5\n",
        "        print(f\"  - {vf}\")\n",
        "    if len(video_files) > 5:\n",
        "        print(f\"  ... and {len(video_files) - 5} more\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  No video files found in {VIDEO_DIRECTORY}\")\n",
        "    print(f\"\\nüì§ To upload videos, you can:\")\n",
        "    print(f\"   1. Use the file upload widget in the next cell\")\n",
        "    print(f\"   2. Drag and drop files to the Colab file browser sidebar\")\n",
        "    print(f\"   3. Upload to Google Drive and access via mounted drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload video files (optional - if you haven't uploaded them yet)\n",
        "# Uncomment the code below to use the file upload widget\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"üì§ Upload your video files:\")\n",
        "print(\"   Click 'Choose Files' and select your video files\")\n",
        "print(\"   Files will be saved to:\", VIDEO_DIRECTORY)\n",
        "\n",
        "# Uncomment the next two lines to enable file upload:\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     shutil.move(filename, os.path.join(VIDEO_DIRECTORY, filename))\n",
        "#     print(f\"‚úÖ Saved: {filename}\")\n",
        "\n",
        "print(\"\\nüí° Tip: You can also drag and drop files directly in the Colab file browser!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the dubbing process\n",
        "# This will process all videos in the specified directory\n",
        "\n",
        "print(\"üöÄ Starting dubbing process...\")\n",
        "print(\"This may take a while, especially for the first run (model downloads)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "results = process_videos(\n",
        "    video_directory=VIDEO_DIRECTORY,\n",
        "    source_language=SOURCE_LANGUAGE,\n",
        "    target_language=TARGET_LANGUAGE,\n",
        "    output_directory=OUTPUT_DIRECTORY\n",
        ")\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    status = \"‚úÖ SUCCESS\" if result.get('success') else \"‚ùå FAILED\"\n",
        "    video_name = result.get('video', 'Unknown')\n",
        "    if isinstance(video_name, str):\n",
        "        video_name = Path(video_name).name\n",
        "    print(f\"{i}. {video_name}: {status}\")\n",
        "    if result.get('success') and 'final_output_path' in result:\n",
        "        print(f\"   Output: {result['final_output_path']}\")\n",
        "    elif 'error' in result:\n",
        "        print(f\"   Error: {result['error']}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Results\n",
        "\n",
        "After processing, you can download the dubbed videos from Colab or access them in your Google Drive (if you mounted it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download all dubbed videos as a ZIP file\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Find output directory\n",
        "if OUTPUT_DIRECTORY:\n",
        "    output_dir = Path(OUTPUT_DIRECTORY)\n",
        "else:\n",
        "    output_dir = Path(VIDEO_DIRECTORY) / \"dubbed\"\n",
        "\n",
        "if output_dir.exists():\n",
        "    zip_path = Path(\"/content/dubbed_videos.zip\")\n",
        "    \n",
        "    # Create ZIP file\n",
        "    with ZipFile(zip_path, 'w') as zipf:\n",
        "        for video_file in output_dir.glob(\"*_dubbed.*\"):\n",
        "            zipf.write(video_file, video_file.name)\n",
        "    \n",
        "    print(f\"‚úÖ Created ZIP file: {zip_path}\")\n",
        "    print(f\"   Contains {len(list(output_dir.glob('*_dubbed.*')))} dubbed video(s)\")\n",
        "    print(f\"\\nüì• Downloading the ZIP file...\")\n",
        "    \n",
        "    # Download the ZIP file\n",
        "    files.download(str(zip_path))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Output directory not found: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Help\n",
        "\n",
        "### Supported Languages\n",
        "\n",
        "**Source Languages:** Any language supported by Whisper (e.g., 'en', 'fr', 'es', 'de', 'ru', 'zh', 'ja', 'ko', etc.)\n",
        "\n",
        "**Target Languages:**\n",
        "- **Nigerian Languages:** 'yo' (Yoruba), 'ig' (Igbo), 'ha' (Hausa)\n",
        "- **Other Languages:** 'en', 'fr', 'es', 'de', 'ru', 'zh', 'sw', etc.\n",
        "\n",
        "### Notes:\n",
        "- First run will download models (NLLB-200 ~2.5GB, Whisper, Demucs)\n",
        "- Processing time depends on video length and complexity\n",
        "- GPU is recommended but not required\n",
        "- Output videos will be saved with \"_dubbed\" suffix"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
