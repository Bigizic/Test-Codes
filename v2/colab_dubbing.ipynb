{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "%pip install -q torch torchaudio transformers openai-whisper demucs moviepy pydub pyrubberband python-dotenv TTS\n",
        "\n",
        "# Install system dependencies for audio processing\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg sox\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Dubbing Service V2 - Google Colab Edition (Yoruba Focus)\n",
        "\n",
        "This notebook replicates the complete v2 dubbing experience for Yoruba language.\n",
        "\n",
        "## Features:\n",
        "- Complete dubbing pipeline (transcription ‚Üí translation ‚Üí TTS ‚Üí assembly)\n",
        "- Yoruba language support (can be extended to other languages)\n",
        "- All processing done on Colab runtime (no Drive mounting needed)\n",
        "- Models downloaded automatically (~3GB total)\n",
        "\n",
        "## Setup:\n",
        "1. Run Cell 1 to install dependencies\n",
        "2. Run all cells sequentially\n",
        "3. Upload your video to `/content/videos/` or provide a video directory path\n",
        "4. All outputs saved to `/content/output/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment and paths\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set working directory to /content\n",
        "WORK_DIR = Path('/content')\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "# Create directories for videos and output\n",
        "os.makedirs('/content/videos', exist_ok=True)\n",
        "os.makedirs('/content/output', exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Environment setup complete\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"üìÅ Video directory: /content/videos\")\n",
        "print(f\"üìÅ Output directory: /content/output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries and setup logging\n",
        "import asyncio\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import uuid\n",
        "import io\n",
        "import shutil\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Audio/Video processing\n",
        "from moviepy import VideoFileClip, AudioFileClip\n",
        "from pydub import AudioSegment\n",
        "import soundfile as sf\n",
        "\n",
        "# ML/AI libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "try:\n",
        "    import whisper\n",
        "    WHISPER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WHISPER_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from demucs import pretrained\n",
        "    from demucs.apply import apply_model\n",
        "    DEMUCS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DEMUCS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoProcessor, AutoModel\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from TTS.api import TTS as CoquiTTS\n",
        "    COQUI_TTS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    COQUI_TTS_AVAILABLE = False\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translation Tools - NLLB Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation Tools - NLLB Translator (for Yoruba and other languages)\n",
        "NLLB_LANGUAGE_MAP = {\n",
        "    'en': 'eng_Latn', 'ha': 'hau_Latn', 'ig': 'ibo_Latn', 'yo': 'yor_Latn',\n",
        "    'fr': 'fra_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn', 'ru': 'rus_Cyrl',\n",
        "    'zh': 'zho_Hans', 'sw': 'swh_Latn',\n",
        "}\n",
        "NLLB_MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "\n",
        "class NLLBTranslator:\n",
        "    \"\"\"Translation service using Facebook NLLB-200 model\"\"\"\n",
        "    _model = None\n",
        "    _tokenizer = None\n",
        "    _device = None\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_device(cls):\n",
        "        if cls._device:\n",
        "            return cls._device\n",
        "        if torch.cuda.is_available():\n",
        "            cls._device = torch.device(\"cuda\")\n",
        "            logger.info(\"Using CUDA device for NLLB translation\")\n",
        "        else:\n",
        "            cls._device = torch.device(\"cpu\")\n",
        "            logger.info(\"Using CPU device for NLLB translation\")\n",
        "        return cls._device\n",
        "    \n",
        "    @classmethod\n",
        "    def _load_model(cls):\n",
        "        if cls._model is not None:\n",
        "            return cls._model, cls._tokenizer\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            raise ImportError(\"transformers not available\")\n",
        "        logger.info(f\"Loading NLLB model: {NLLB_MODEL_NAME}\")\n",
        "        device = cls._get_device()\n",
        "        cls._tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL_NAME, src_lang=\"eng_Latn\")\n",
        "        cls._model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL_NAME).to(device)\n",
        "        cls._model.eval()\n",
        "        logger.info(\"NLLB-200 model loaded successfully\")\n",
        "        return cls._model, cls._tokenizer\n",
        "    \n",
        "    @classmethod\n",
        "    def translate(cls, text: str, source_language: str, target_language: str, max_length: int = 512) -> str:\n",
        "        if not text or not text.strip():\n",
        "            return text\n",
        "        model, tokenizer = cls._load_model()\n",
        "        device = cls._get_device()\n",
        "        src_lang_code = NLLB_LANGUAGE_MAP.get(source_language.lower(), 'eng_Latn')\n",
        "        tgt_lang_code = NLLB_LANGUAGE_MAP.get(target_language.lower(), 'eng_Latn')\n",
        "        tokenizer.src_lang = src_lang_code\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang_code], max_length=max_length, num_beams=4, early_stopping=True)\n",
        "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "        return translated_text.strip()\n",
        "\n",
        "print(\"‚úÖ Translation tools loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TTS Tools - Coqui XTTS and Hugging Face TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTS Tools - Coqui XTTS and helpers\n",
        "TTS_LANGUAGE_MAP = {'en': 'en', 'ha': 'ha', 'ig': 'ig', 'yo': 'yo', 'fr': 'fr', 'es': 'es', 'de': 'de', 'ru': 'ru', 'zh': 'zh', 'sw': 'sw'}\n",
        "COQUI_XTTS_MODEL = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "\n",
        "class CoquiXTTS:\n",
        "    \"\"\"Text-to-speech using Coqui XTTS-v2 (multilingual, best quality)\"\"\"\n",
        "    _tts_model = None\n",
        "    _device = None\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_device(cls):\n",
        "        if cls._device:\n",
        "            return cls._device\n",
        "        cls._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        return cls._device\n",
        "    \n",
        "    @classmethod\n",
        "    def _load_model(cls):\n",
        "        if cls._tts_model is not None:\n",
        "            return cls._tts_model\n",
        "        if not COQUI_TTS_AVAILABLE:\n",
        "            raise ImportError(\"Coqui TTS not available\")\n",
        "        logger.info(\"Loading Coqui XTTS-v2 model\")\n",
        "        cls._tts_model = CoquiTTS(model_name=COQUI_XTTS_MODEL, progress_bar=False)\n",
        "        cls._tts_model.to(cls._get_device())\n",
        "        logger.info(\"Coqui XTTS-v2 model loaded successfully\")\n",
        "        return cls._tts_model\n",
        "    \n",
        "    @classmethod\n",
        "    def synthesize(cls, text: str, language: str, speaker_wav: Optional[str] = None) -> AudioSegment:\n",
        "        if not text or not text.strip():\n",
        "            raise ValueError(\"Empty text\")\n",
        "        tts_model = cls._load_model()\n",
        "        lang_code = TTS_LANGUAGE_MAP.get(language.lower(), 'en')\n",
        "        output_path = os.path.join(tempfile.gettempdir(), f\"temp_tts_{hash(text) % 10000}.wav\")\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        try:\n",
        "            if speaker_wav and os.path.exists(speaker_wav):\n",
        "                tts_model.tts_to_file(text=text, file_path=output_path, language=lang_code, speaker_wav=speaker_wav)\n",
        "            else:\n",
        "                try:\n",
        "                    tts_model.tts_to_file(text=text, file_path=output_path, language=lang_code)\n",
        "                except:\n",
        "                    tts_model.tts_to_file(text=text, file_path=output_path)\n",
        "            audio = AudioSegment.from_wav(output_path)\n",
        "            try:\n",
        "                os.remove(output_path)\n",
        "            except:\n",
        "                pass\n",
        "            return audio\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Coqui TTS failed: {e}\")\n",
        "            raise\n",
        "\n",
        "def generate_tts_huggingface(text: str, language: str, prefer_coqui: bool = True) -> AudioSegment:\n",
        "    \"\"\"Generate TTS using best available Hugging Face model (for Yoruba, uses Coqui XTTS-v2)\"\"\"\n",
        "    lang = language.lower()\n",
        "    if prefer_coqui and COQUI_TTS_AVAILABLE:\n",
        "        logger.info(f\"Using Coqui XTTS-v2 for {language} (multilingual model)\")\n",
        "        return CoquiXTTS.synthesize(text, language)\n",
        "    else:\n",
        "        raise ImportError(\"No TTS available. Install: pip install TTS\")\n",
        "\n",
        "print(\"‚úÖ TTS tools loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Service Classes - All processors inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Service Classes - All service processors inline\n",
        "\n",
        "class VideoProcessor:\n",
        "    \"\"\"Video and audio processing service\"\"\"\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self._demucs_model = None\n",
        "    \n",
        "    async def extract_audio(self, video_path: str) -> str:\n",
        "        logger.info(f\"[JOB {self.job_id}] Extracting audio from video\")\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        if audio is None:\n",
        "            raise Exception(\"No audio track found\")\n",
        "        audio_path = os.path.join(output_dir, \"original_audio.wav\")\n",
        "        audio.write_audiofile(audio_path, fps=22050, nbytes=2, logger=None)\n",
        "        test_audio = AudioSegment.from_file(audio_path)\n",
        "        if test_audio.channels == 1:\n",
        "            stereo_audio = test_audio.set_channels(2)\n",
        "            stereo_audio.export(audio_path, format=\"wav\")\n",
        "        audio.close()\n",
        "        video.close()\n",
        "        return audio_path\n",
        "    \n",
        "    async def separate_audio(self, audio_path: str) -> Dict[str, str]:\n",
        "        if not DEMUCS_AVAILABLE:\n",
        "            raise Exception(\"Demucs not available\")\n",
        "        if self._demucs_model is None:\n",
        "            logger.info(f\"[JOB {self.job_id}] Loading Demucs model...\")\n",
        "            self._demucs_model = pretrained.get_model('htdemucs')\n",
        "            self._demucs_model.eval()\n",
        "        wav, sr = torchaudio.load(audio_path)\n",
        "        if sr != self._demucs_model.samplerate:\n",
        "            wav = torchaudio.functional.resample(wav, sr, self._demucs_model.samplerate)\n",
        "            sr = self._demucs_model.samplerate\n",
        "        if wav.dim() == 2:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        elif wav.dim() == 1:\n",
        "            wav = wav.unsqueeze(0).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            sources = apply_model(self._demucs_model, wav, device='cpu', progress=True)\n",
        "        source_names = ['drums', 'bass', 'other', 'vocals']\n",
        "        separated_files = {}\n",
        "        output_dir = os.path.dirname(audio_path)\n",
        "        for i, name in enumerate(source_names):\n",
        "            output_path = os.path.join(output_dir, f\"{name}.wav\")\n",
        "            source_audio = sources[0, i].cpu().numpy()\n",
        "            if len(source_audio.shape) == 1:\n",
        "                source_audio = np.stack([source_audio, source_audio])\n",
        "            sf.write(output_path, source_audio.T, sr)\n",
        "            separated_files[name] = output_path\n",
        "        return separated_files\n",
        "    \n",
        "    async def create_background(self, separated_files: Dict[str, str]) -> str:\n",
        "        output_dir = os.path.dirname(separated_files['drums'])\n",
        "        background_path = os.path.join(output_dir, \"background.wav\")\n",
        "        drums, sr = sf.read(separated_files['drums'])\n",
        "        bass, _ = sf.read(separated_files['bass'])\n",
        "        other, _ = sf.read(separated_files['other'])\n",
        "        background = drums + bass + other\n",
        "        sf.write(background_path, background, sr)\n",
        "        return background_path\n",
        "    \n",
        "    async def replace_audio(self, video_path: str, audio_path: str) -> str:\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, \"dubbed_video.mp4\")\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio_clip = AudioFileClip(audio_path)\n",
        "        if audio_clip.duration < video.duration:\n",
        "            audio_clip = audio_clip.set_duration(video.duration)\n",
        "        elif audio_clip.duration > video.duration:\n",
        "            audio_clip = audio_clip.subclipped(0, video.duration)\n",
        "        final_video = video.with_audio(audio_clip)\n",
        "        final_video.write_videofile(output_path, codec='libx264', audio_codec='aac', logger=None)\n",
        "        video.close()\n",
        "        audio_clip.close()\n",
        "        final_video.close()\n",
        "        return output_path\n",
        "\n",
        "class TranscriptionProcessor:\n",
        "    \"\"\"Audio transcription service\"\"\"\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self._whisper_model = None\n",
        "    \n",
        "    async def transcribe(self, audio_path: str, language: str) -> List[Dict[str, Any]]:\n",
        "        if not WHISPER_AVAILABLE:\n",
        "            raise Exception(\"Whisper not available\")\n",
        "        if self._whisper_model is None:\n",
        "            logger.info(f\"[JOB {self.job_id}] Loading Whisper model...\")\n",
        "            self._whisper_model = whisper.load_model(\"base\")\n",
        "        result = self._whisper_model.transcribe(audio_path, language=language if language != \"en\" else None, task=\"transcribe\", fp16=False)\n",
        "        segments = []\n",
        "        for seg in result.get(\"segments\", []):\n",
        "            segments.append({\"start_time\": seg.get(\"start\", 0), \"end_time\": seg.get(\"end\", 0), \"duration\": seg.get(\"end\", 0) - seg.get(\"start\", 0), \"transcription\": seg.get(\"text\", \"\").strip()})\n",
        "        return segments\n",
        "\n",
        "print(\"‚úÖ VideoProcessor and TranscriptionProcessor loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation and TTS Processors (with Yoruba-specific support)\n",
        "\n",
        "class TranslationProcessor:\n",
        "    \"\"\"Text translation service - uses NLLB-200, with Yoruba-specific handling\"\"\"\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        logger.info(f\"[JOB {self.job_id}] Initialized translation processor (NLLB-200)\")\n",
        "    \n",
        "    async def translate(self, segments: List[Dict[str, Any]], source_language: str, target_language: str) -> List[Dict[str, Any]]:\n",
        "        logger.info(f\"[JOB {self.job_id}] Translating {len(segments)} segments: {source_language} ‚Üí {target_language}\")\n",
        "        translated_segments = []\n",
        "        for i, seg in enumerate(segments):\n",
        "            text = seg.get(\"transcription\", \"\")\n",
        "            if not text:\n",
        "                seg[\"translated_text\"] = \"\"\n",
        "                translated_segments.append(seg)\n",
        "                continue\n",
        "            # Use NLLB-200 for translation (especially good for Yoruba)\n",
        "            if target_language in ['yo', 'ig', 'ha'] and TRANSFORMERS_AVAILABLE:\n",
        "                try:\n",
        "                    translated_text = NLLBTranslator.translate(text, source_language, target_language)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"[JOB {self.job_id}] Translation failed: {e}, using original\")\n",
        "                    translated_text = text\n",
        "            else:\n",
        "                translated_text = text\n",
        "            seg[\"translated_text\"] = translated_text\n",
        "            translated_segments.append(seg)\n",
        "            if (i + 1) % 10 == 0:\n",
        "                logger.info(f\"[JOB {self.job_id}] Translated {i + 1}/{len(segments)} segments\")\n",
        "        return translated_segments\n",
        "\n",
        "class TTSProcessor:\n",
        "    \"\"\"Text-to-speech service - uses Coqui XTTS-v2, with Yoruba-specific handling\"\"\"\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        logger.info(f\"[JOB {self.job_id}] Initialized TTS processor (Coqui XTTS-v2)\")\n",
        "    \n",
        "    async def generate_tts(self, segments: List[Dict[str, Any]], target_language: str) -> List[Dict[str, Any]]:\n",
        "        logger.info(f\"[JOB {self.job_id}] Generating TTS for {len(segments)} segments in {target_language}\")\n",
        "        tts_segments = []\n",
        "        for i, seg in enumerate(segments):\n",
        "            text = seg.get(\"translated_text\", \"\")\n",
        "            if not text:\n",
        "                seg[\"audio\"] = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "                tts_segments.append(seg)\n",
        "                continue\n",
        "            # Use Hugging Face TTS (Coqui XTTS-v2) for Yoruba and other languages\n",
        "            if target_language in ['yo', 'ig', 'ha'] and COQUI_TTS_AVAILABLE:\n",
        "                try:\n",
        "                    audio = generate_tts_huggingface(text, target_language, prefer_coqui=True)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"[JOB {self.job_id}] TTS failed: {e}, using silence\")\n",
        "                    audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            elif COQUI_TTS_AVAILABLE:\n",
        "                try:\n",
        "                    audio = CoquiXTTS.synthesize(text, target_language)\n",
        "                except:\n",
        "                    audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            else:\n",
        "                audio = AudioSegment.silent(duration=int(seg.get(\"duration\", 0) * 1000))\n",
        "            seg[\"audio\"] = audio\n",
        "            tts_segments.append(seg)\n",
        "            if (i + 1) % 10 == 0:\n",
        "                logger.info(f\"[JOB {self.job_id}] Generated TTS for {i + 1}/{len(segments)} segments\")\n",
        "        return tts_segments\n",
        "\n",
        "print(\"‚úÖ TranslationProcessor and TTSProcessor loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audio Assembler - Assembles TTS segments with background audio\n",
        "\n",
        "class AudioAssembler:\n",
        "    \"\"\"Audio assembly service with natural pacing\"\"\"\n",
        "    def __init__(self, job_id: str):\n",
        "        self.job_id = job_id\n",
        "        self.MIN_PAUSE_BETWEEN_SEGMENTS = 0.3\n",
        "        self.IDEAL_PAUSE_BETWEEN_SEGMENTS = 0.5\n",
        "        self.FADE_IN_DURATION = 50\n",
        "        self.FADE_OUT_DURATION = 100\n",
        "    \n",
        "    def _add_fade_effects(self, audio: AudioSegment) -> AudioSegment:\n",
        "        return audio.fade_in(self.FADE_IN_DURATION).fade_out(self.FADE_OUT_DURATION)\n",
        "    \n",
        "    def _add_natural_pause(self, audio: AudioSegment, pause_duration_ms: int) -> AudioSegment:\n",
        "        if pause_duration_ms > 0:\n",
        "            return audio + AudioSegment.silent(duration=pause_duration_ms)\n",
        "        return audio\n",
        "    \n",
        "    def _calculate_non_overlapping_positions(self, tts_segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        positioned_segments = []\n",
        "        current_end_time = 0.0\n",
        "        for i, seg in enumerate(tts_segments):\n",
        "            if \"audio\" not in seg:\n",
        "                positioned_segments.append(seg)\n",
        "                continue\n",
        "            audio = seg[\"audio\"]\n",
        "            audio_duration = len(audio) / 1000.0\n",
        "            original_start = seg.get(\"start_time\", 0)\n",
        "            if i == 0:\n",
        "                final_start = max(0.0, original_start)\n",
        "            else:\n",
        "                min_start = current_end_time + self.MIN_PAUSE_BETWEEN_SEGMENTS\n",
        "                final_start = max(original_start, min_start)\n",
        "            pause_duration_ms = int(self.IDEAL_PAUSE_BETWEEN_SEGMENTS * 1000)\n",
        "            audio_with_pause = self._add_natural_pause(audio, pause_duration_ms)\n",
        "            seg_copy = seg.copy()\n",
        "            seg_copy[\"audio\"] = audio_with_pause\n",
        "            seg_copy[\"final_start_time\"] = final_start\n",
        "            seg_copy[\"final_end_time\"] = final_start + (len(audio_with_pause) / 1000.0)\n",
        "            positioned_segments.append(seg_copy)\n",
        "            current_end_time = seg_copy[\"final_end_time\"]\n",
        "        return positioned_segments\n",
        "    \n",
        "    async def assemble(self, tts_segments: List[Dict[str, Any]], background_path: str) -> str:\n",
        "        background = AudioSegment.from_file(background_path)\n",
        "        background_duration = len(background) / 1000.0\n",
        "        positioned_segments = self._calculate_non_overlapping_positions(tts_segments)\n",
        "        max_end_time = 0.0\n",
        "        for seg in positioned_segments:\n",
        "            end_time = seg.get(\"final_end_time\", seg.get(\"end_time\", 0))\n",
        "            max_end_time = max(max_end_time, end_time)\n",
        "        total_duration = max(background_duration, max_end_time)\n",
        "        total_duration_ms = int(total_duration * 1000)\n",
        "        assembled_audio = AudioSegment.silent(duration=total_duration_ms)\n",
        "        assembled_audio = assembled_audio.overlay(background, gain_during_overlay=-6)\n",
        "        for seg in positioned_segments:\n",
        "            if \"audio\" not in seg:\n",
        "                continue\n",
        "            audio = seg[\"audio\"]\n",
        "            final_start = seg.get(\"final_start_time\", seg.get(\"start_time\", 0))\n",
        "            audio = self._add_fade_effects(audio)\n",
        "            start_ms = int(final_start * 1000)\n",
        "            if start_ms + len(audio) > total_duration_ms:\n",
        "                max_audio_length = total_duration_ms - start_ms\n",
        "                if max_audio_length > 0:\n",
        "                    audio = audio[:max_audio_length]\n",
        "                else:\n",
        "                    continue\n",
        "            assembled_audio = assembled_audio.overlay(audio, position=start_ms, gain_during_overlay=+3)\n",
        "        output_dir = os.path.join(tempfile.gettempdir(), \"v2_dubbing\", f\"job_{self.job_id}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        final_audio_path = os.path.join(output_dir, \"final_audio.mp3\")\n",
        "        assembled_audio.export(final_audio_path, format=\"mp3\", bitrate=\"320k\")\n",
        "        return final_audio_path\n",
        "\n",
        "print(\"‚úÖ AudioAssembler loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DubbingProcessor - Main orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DubbingProcessor - Main processor class (replicates v2/services/dubbing_processor.py)\n",
        "\n",
        "class DubbingProcessor:\n",
        "    \"\"\"Main processor for video dubbing workflow\"\"\"\n",
        "    def __init__(self):\n",
        "        self.job_id = f\"job_{uuid.uuid4().hex[:12]}\"\n",
        "        self.temp_files = []\n",
        "        \n",
        "    async def process_video(self, video_path: str, source_language: str, target_language: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            logger.info(f\"[JOB {self.job_id}] Starting dubbing process\")\n",
        "            logger.info(f\"[JOB {self.job_id}] Video: {video_path}\")\n",
        "            logger.info(f\"[JOB {self.job_id}] Languages: {source_language} ‚Üí {target_language}\")\n",
        "            \n",
        "            # Phase 1: Extract audio from video\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 1: Extracting audio\")\n",
        "            video_processor = VideoProcessor(self.job_id)\n",
        "            audio_path = await video_processor.extract_audio(video_path)\n",
        "            self.temp_files.append(audio_path)\n",
        "            \n",
        "            # Phase 2: Separate audio stems\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 2: Separating audio\")\n",
        "            separated_files = await video_processor.separate_audio(audio_path)\n",
        "            self.temp_files.extend(separated_files.values())\n",
        "            \n",
        "            # Phase 3: Create background track\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 3: Creating background track\")\n",
        "            background_path = await video_processor.create_background(separated_files)\n",
        "            self.temp_files.append(background_path)\n",
        "            \n",
        "            # Phase 4: Transcribe audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 4: Transcribing audio\")\n",
        "            transcription_processor = TranscriptionProcessor(self.job_id)\n",
        "            segments = await transcription_processor.transcribe(separated_files['vocals'], source_language)\n",
        "            \n",
        "            # Phase 5: Translate segments\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 5: Translating segments\")\n",
        "            translation_processor = TranslationProcessor(self.job_id)\n",
        "            translated_segments = await translation_processor.translate(segments, source_language, target_language)\n",
        "            \n",
        "            # Phase 6: Generate TTS\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 6: Generating TTS\")\n",
        "            tts_processor = TTSProcessor(self.job_id)\n",
        "            tts_segments = await tts_processor.generate_tts(translated_segments, target_language)\n",
        "            \n",
        "            # Phase 7: Assemble final audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 7: Assembling final audio\")\n",
        "            assembler = AudioAssembler(self.job_id)\n",
        "            final_audio_path = await assembler.assemble(tts_segments, background_path)\n",
        "            self.temp_files.append(final_audio_path)\n",
        "            \n",
        "            # Phase 8: Replace video audio\n",
        "            logger.info(f\"[JOB {self.job_id}] Phase 8: Replacing video audio\")\n",
        "            output_path = await video_processor.replace_audio(video_path, final_audio_path)\n",
        "            \n",
        "            logger.info(f\"[JOB {self.job_id}] ‚úÖ Dubbing complete: {output_path}\")\n",
        "            return {\"success\": True, \"output_path\": output_path, \"job_id\": self.job_id}\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[JOB {self.job_id}] ‚ùå Dubbing failed: {e}\", exc_info=True)\n",
        "            return {\"success\": False, \"error\": str(e), \"job_id\": self.job_id}\n",
        "\n",
        "print(\"‚úÖ DubbingProcessor loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main processing functions - replicate v2 experience\n",
        "\n",
        "async def process_video_directory(\n",
        "    video_directory: str,\n",
        "    source_language: str,\n",
        "    target_language: str,\n",
        "    output_directory: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Process all videos in a directory through the dubbing pipeline\n",
        "    Replicates the exact v2 dubbing experience\n",
        "    \n",
        "    Args:\n",
        "        video_directory: Path to directory containing video files\n",
        "        source_language: Source language code (e.g., 'en', 'fr', 'es')\n",
        "        target_language: Target language code (e.g., 'yo' for Yoruba)\n",
        "        output_directory: Optional output directory (defaults to /content/output)\n",
        "    \n",
        "    Returns:\n",
        "        List of results for each processed video\n",
        "    \"\"\"\n",
        "    video_dir = Path(video_directory)\n",
        "    if not video_dir.exists():\n",
        "        raise ValueError(f\"Video directory not found: {video_directory}\")\n",
        "    \n",
        "    # Set output directory (use /content/output by default)\n",
        "    if output_directory is None:\n",
        "        output_dir = Path('/content/output')\n",
        "    else:\n",
        "        output_dir = Path(output_directory)\n",
        "    \n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Find all video files\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v']\n",
        "    video_files = []\n",
        "    for ext in video_extensions:\n",
        "        video_files.extend(video_dir.glob(f\"*{ext}\"))\n",
        "        video_files.extend(video_dir.glob(f\"*{ext.upper()}\"))\n",
        "    \n",
        "    if not video_files:\n",
        "        logger.warning(f\"No video files found in {video_directory}\")\n",
        "        return []\n",
        "    \n",
        "    logger.info(f\"Found {len(video_files)} video file(s) to process\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Process each video\n",
        "    for video_path in video_files:\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Processing: {video_path.name}\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "        \n",
        "        try:\n",
        "            # Create dubbing processor\n",
        "            processor = DubbingProcessor()\n",
        "            \n",
        "            # Process video\n",
        "            result = await processor.process_video(\n",
        "                str(video_path),\n",
        "                source_language,\n",
        "                target_language\n",
        "            )\n",
        "            \n",
        "            if result['success']:\n",
        "                # Move output to designated output directory\n",
        "                output_path = Path(result['output_path'])\n",
        "                final_output = output_dir / f\"{video_path.stem}_dubbed{video_path.suffix}\"\n",
        "                \n",
        "                # Copy the file\n",
        "                shutil.copy2(output_path, final_output)\n",
        "                logger.info(f\"‚úÖ Saved dubbed video to: {final_output}\")\n",
        "                \n",
        "                result['final_output_path'] = str(final_output)\n",
        "                results.append(result)\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Failed to process {video_path.name}: {result.get('error', 'Unknown error')}\")\n",
        "                results.append(result)\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error processing {video_path.name}: {e}\", exc_info=True)\n",
        "            results.append({\n",
        "                'success': False,\n",
        "                'video': str(video_path),\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Processing complete! Processed {len([r for r in results if r.get('success')])}/{len(video_files)} videos\")\n",
        "    logger.info(f\"Output directory: {output_dir}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def process_videos(\n",
        "    video_directory: str,\n",
        "    source_language: str,\n",
        "    target_language: str,\n",
        "    output_directory: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Synchronous wrapper for process_video_directory\n",
        "    Use this function directly in Colab cells\n",
        "    \"\"\"\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        results = loop.run_until_complete(\n",
        "            process_video_directory(\n",
        "                video_directory,\n",
        "                source_language,\n",
        "                target_language,\n",
        "                output_directory\n",
        "            )\n",
        "        )\n",
        "        return results\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "print(\"‚úÖ Processing functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your video directory path and language settings here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION - Adjust these parameters\n",
        "\n",
        "# Path to directory containing your video files (or single video file path)\n",
        "VIDEO_DIRECTORY = \"/content/videos\"  # Change this to your video directory path\n",
        "\n",
        "# Option: Video URL (if you want to download from URL)\n",
        "VIDEO_URL = None  # e.g., \"https://example.com/video.mp4\"\n",
        "\n",
        "# Source language (language spoken in the video)\n",
        "SOURCE_LANGUAGE = \"en\"  # Options: 'en', 'fr', 'es', 'de', 'ru', 'zh', etc.\n",
        "\n",
        "# Target language (language you want to dub to)\n",
        "TARGET_LANGUAGE = \"yo\"  # 'yo' for Yoruba (focus language)\n",
        "\n",
        "# Output directory (defaults to /content/output)\n",
        "OUTPUT_DIRECTORY = \"/content/output\"  # All outputs saved here\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Video Directory: {VIDEO_DIRECTORY}\")\n",
        "if VIDEO_URL:\n",
        "    print(f\"  Video URL: {VIDEO_URL}\")\n",
        "print(f\"  Source Language: {SOURCE_LANGUAGE}\")\n",
        "print(f\"  Target Language: {TARGET_LANGUAGE} (Yoruba)\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIRECTORY}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download video from URL or prepare video directory\n",
        "os.makedirs(VIDEO_DIRECTORY, exist_ok=True)\n",
        "\n",
        "# Option 1: Download video from URL\n",
        "if VIDEO_URL:\n",
        "    print(f\"üì• Downloading video from URL: {VIDEO_URL}\")\n",
        "    try:\n",
        "        response = requests.get(VIDEO_URL, stream=True)\n",
        "        response.raise_for_status()\n",
        "        filename = os.path.basename(VIDEO_URL.split('?')[0]) or \"video.mp4\"\n",
        "        if not filename.endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v')):\n",
        "            filename = \"video.mp4\"\n",
        "        video_path = os.path.join(VIDEO_DIRECTORY, filename)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        downloaded = 0\n",
        "        with open(video_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    downloaded += len(chunk)\n",
        "                    if total_size > 0:\n",
        "                        percent = (downloaded / total_size) * 100\n",
        "                        print(f\"\\r   Progress: {percent:.1f}%\", end='', flush=True)\n",
        "        print(f\"\\n‚úÖ Video downloaded: {video_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading video: {e}\")\n",
        "\n",
        "# List existing video files\n",
        "video_files = []\n",
        "if os.path.exists(VIDEO_DIRECTORY):\n",
        "    video_files = [f for f in os.listdir(VIDEO_DIRECTORY) \n",
        "                   if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v'))]\n",
        "\n",
        "if video_files:\n",
        "    print(f\"\\nüìπ Found {len(video_files)} video file(s) in {VIDEO_DIRECTORY}:\")\n",
        "    for vf in video_files[:5]:\n",
        "        print(f\"  - {vf}\")\n",
        "    if len(video_files) > 5:\n",
        "        print(f\"  ... and {len(video_files) - 5} more\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  No video files found in {VIDEO_DIRECTORY}\")\n",
        "    print(f\"üì§ To upload videos:\")\n",
        "    print(f\"   1. Drag and drop files in the Colab file browser sidebar\")\n",
        "    print(f\"   2. Or use: from google.colab import files; files.upload()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Dubbing Process\n",
        "\n",
        "This will process all videos in the specified directory through the complete dubbing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the dubbing process\n",
        "# This replicates the exact v2 dubbing experience\n",
        "\n",
        "print(\"üöÄ Starting dubbing process...\")\n",
        "print(\"This may take a while, especially for the first run (model downloads ~3GB)\")\n",
        "print(\"Models will be downloaded to Colab runtime storage\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "results = process_videos(\n",
        "    video_directory=VIDEO_DIRECTORY,\n",
        "    source_language=SOURCE_LANGUAGE,\n",
        "    target_language=TARGET_LANGUAGE,\n",
        "    output_directory=OUTPUT_DIRECTORY\n",
        ")\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    status = \"‚úÖ SUCCESS\" if result.get('success') else \"‚ùå FAILED\"\n",
        "    video_name = result.get('video', 'Unknown')\n",
        "    if isinstance(video_name, str):\n",
        "        video_name = Path(video_name).name\n",
        "    print(f\"{i}. {video_name}: {status}\")\n",
        "    if result.get('success') and 'final_output_path' in result:\n",
        "        print(f\"   Output: {result['final_output_path']}\")\n",
        "    elif 'error' in result:\n",
        "        print(f\"   Error: {result['error']}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìÅ All output videos saved to: {OUTPUT_DIRECTORY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Results\n",
        "\n",
        "Download your dubbed videos from the Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download all dubbed videos as a ZIP file\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "\n",
        "output_dir = Path(OUTPUT_DIRECTORY)\n",
        "\n",
        "if output_dir.exists():\n",
        "    zip_path = Path(\"/content/dubbed_videos.zip\")\n",
        "    \n",
        "    # Create ZIP file\n",
        "    with ZipFile(zip_path, 'w') as zipf:\n",
        "        for video_file in output_dir.glob(\"*_dubbed.*\"):\n",
        "            zipf.write(video_file, video_file.name)\n",
        "    \n",
        "    print(f\"‚úÖ Created ZIP file: {zip_path}\")\n",
        "    print(f\"   Contains {len(list(output_dir.glob('*_dubbed.*')))} dubbed video(s)\")\n",
        "    print(f\"\\nüì• Downloading the ZIP file...\")\n",
        "    \n",
        "    # Download the ZIP file\n",
        "    files.download(str(zip_path))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Output directory not found: {output_dir}\")\n",
        "    print(f\"   Check that processing completed successfully\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
